name: Daily Scraper Delhi

on:
  schedule:
    - cron: '0 1 * * *'   # Runs daily at 6:30 AM IST → 1:00 AM UTC
  workflow_dispatch:

jobs:
  run-script:
    runs-on: ubuntu-latest

    steps:

    # ✅ Step 1: Setup SSH
    - name: Setup SSH
      run: |
        mkdir -p ~/.ssh
        echo "${{ secrets.PRIVATE_KEY }}" > ~/.ssh/id_ed25519
        chmod 600 ~/.ssh/id_ed25519
        eval "$(ssh-agent -s)"
        ssh-add ~/.ssh/id_ed25519
        ssh-keyscan github.com >> ~/.ssh/known_hosts

    # ✅ Step 2: Clone Private Repo
    - name: Clone private repo
      run: |
        git clone git@github.com:LongwayDataTeam/Amz_Flp_Data_Scraper_Delhi.git

    # ✅ Step 3: Set up Python
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: "3.10"

    # ✅ Step 4: Install ONLY Google Chrome (No ChromeDriver)
    - name: Install Google Chrome
      run: |
        sudo apt-get update
        sudo apt-get install -y wget unzip xvfb libnss3 libxss1 libxi6 fonts-liberation libu2f-udev

        wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb
        sudo dpkg -i google-chrome-stable_current_amd64.deb || sudo apt-get -f install -y

        google-chrome --version

    # ✅ Step 5: Install Python Dependencies
    - name: Install Python dependencies
      run: |
        cd Amz_Flp_Data_Scraper_Delhi
        pip install --upgrade pip
        pip install -r requirements.txt

        # ✅ Mandatory for correct ChromeDriver match
        pip install webdriver-manager

    # ✅ Step 6: Run Scraper Script
    - name: Run scraping script
      env:
        GSHEET_CREDENTIALS: ${{ secrets.GSHEET_CREDENTIALS }}
      run: |
        cd Amz_Flp_Data_Scraper_Delhi
        python amz_flp_data_scraper_delhi.py
